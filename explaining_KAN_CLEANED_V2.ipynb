{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import shap\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "from efficient_kan import KAN\n",
    "import collections.abc as c\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                alkane\n",
       "1                methyl\n",
       "2                alkene\n",
       "3                alkyne\n",
       "4              alcohols\n",
       "5                amines\n",
       "6              nitriles\n",
       "7             aromatics\n",
       "8         alkyl halides\n",
       "9                esters\n",
       "10              ketones\n",
       "11            aldehydes\n",
       "12     carboxylic acids\n",
       "13                ether\n",
       "14         acyl halides\n",
       "15               amides\n",
       "16                nitro\n",
       "17         heterocyclic\n",
       "18       aryl chlorides\n",
       "19    carboxylic esters\n",
       "20    alkyl aryl ethers\n",
       "21              phenols\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smarts = {'alkane':'[CX4;H0,H1,H2,H4]',\n",
    "                   'methyl':'[CH3]',\n",
    "                   'alkene':'[CX3]=[CX3]',\n",
    "                   'alkyne':'[CX2]#C',\n",
    "                   'alcohols':'[#6][OX2H]',\n",
    "                   'amines':'[NX3;H2,H1;!$(NC=O)]', \n",
    "                   'nitriles':'[NX1]#[CX2]', \n",
    "                   'aromatics':'[$([cX3](:*):*),$([cX2+](:*):*)]',\n",
    "                   'alkyl halides':'[#6][F,Cl,Br,I]', \n",
    "                   'esters':'[#6][CX3](=O)[OX2H0][#6]', \n",
    "                   'ketones':'[#6][CX3](=O)[#6]',\n",
    "                   'aldehydes':'[CX3H1](=O)[#6]', \n",
    "                   'carboxylic acids':'[CX3](=O)[OX2H1]', \n",
    "                   'ether': '[OD2]([#6])[#6]',\n",
    "                   'acyl halides':'[CX3](=[OX1])[F,Cl,Br,I]',\n",
    "                   'amides':'[NX3][CX3](=[OX1])[#6]',\n",
    "                   'nitro':'[$([NX3](=O)=O),$([NX3+](=O)[O-])][!#8]',\n",
    "                   'heterocyclic': '[!#6;!R0]',\n",
    "                   'aryl chlorides': '[Cl][c]',\n",
    "                   'carboxylic esters': '[CX3;$([R0][#6]),$([H1R0])](=[OX1])[OX2][#6;!$(C=[O,N,S])]',\n",
    "                   'alkyl aryl ethers': '[OX2](c)[CX4;!$(C([OX2])[O,S,#7,#15,F,Cl,Br,I])]',\n",
    "                   'phenols': '[OX2H][c]'}\n",
    "\n",
    "func_group_names = pd.Series(smarts.keys())\n",
    "func_group_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to directiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocDirPath='D:\\\\SHAPE\\\\1VII\\\\functional-cnn-main\\\\ALL\\\\preprocessed_data'\n",
    "funcGroupDirPath='D:\\\\SHAPE\\\\1VII\\\\functional-cnn-main\\\\ALL\\\\functional_groups'\n",
    "save_dir = os.path.join(\"C:\\\\Users\\\\tomek_\\\\Desktop\\\\saved_computations\", \"same_func_group_predictionsTALLKAN1\")\n",
    "model_selection=\"nowy_KAN25VI\"\n",
    "trainedModelDirPath=\"C:\\\\Users\\Tomek_\\\\Desktop\\\\trained_models\\\\\"\n",
    "\n",
    "trained_models_path = os.path.join(trainedModelDirPath, model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionalGroupsDataset(Dataset):\n",
    "    '''\n",
    "    PyTorch compatible dataset of functional groups files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    func_group : str\n",
    "        Name of the functional group to retrieve data from.\n",
    "    \n",
    "    convert_to : str, default None\n",
    "        Converts all data to specified data type.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, func_group: str, convert_to: str = None) -> None:\n",
    "        self.convert_to = convert_to\n",
    "        self.func_group_number = np.where(func_group_names.values == func_group)[0][0]\n",
    "        self.main_dir = os.path.join('..', 'ALL')\n",
    "        self.func_group = func_group\n",
    "\n",
    "        #NIST IDs that passed preprocessing\n",
    "        preprocessed_data_dir = pd.DataFrame(os.listdir(preprocDirPath))\n",
    "\n",
    "\n",
    "        if func_group not in os.listdir(funcGroupDirPath):\n",
    "            raise ValueError(f'{func_group} is not present in our database.')\n",
    "        else:\n",
    "            #All NIST IDs of specific functional group\n",
    "            func_group_data_dir = pd.DataFrame(os.listdir(os.path.join(funcGroupDirPath, func_group)))\n",
    "\n",
    "        #NIST IDs of specific functional group that passed preprocessing\n",
    "        to_sample = pd.merge(preprocessed_data_dir, func_group_data_dir, on = [0, 0], how = 'outer', indicator = True).query('_merge==\"left_only\"')[0]\n",
    "\n",
    "        #Equinumerous dataset of preprocessed functional group NIST IDs and shuffled from every other functional groups\n",
    "        if len(to_sample) < len(func_group_data_dir):\n",
    "            func_group_data_dir = func_group_data_dir.sample(len(to_sample))\n",
    "            self.data = pd.concat([to_sample.sample(len(to_sample)), func_group_data_dir], axis = 0)\n",
    "        else:\n",
    "            self.data = pd.concat([to_sample.sample(len(func_group_data_dir)), func_group_data_dir], axis = 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_path = os.path.join(preprocDirPath, self.data.iloc[index][0])\n",
    "        file = pd.read_csv(file_path)\n",
    "        spectra_type = file['spectraType'][0]\n",
    "    \n",
    "        if not self.convert_to:\n",
    "            spectra = torch.nan_to_num(torch.tensor(file['y'].values, requires_grad=True)).to(torch.float)\n",
    "\n",
    "        elif self.convert_to.lower() not in ('absorbance', 'absorbancja', 'transmittance', 'transmitancja'):\n",
    "            raise ValueError(f'Cant convert to {self.convert_to}.')\n",
    "        \n",
    "        elif str(spectra_type).lower() not in ('converted_to_absorbance', \"absorbance\"):\n",
    "            spectra = torch.nan_to_num(torch.tensor(np.abs(1 - file['y'].values), requires_grad=True)).to(torch.float)\n",
    "        \n",
    "        else:\n",
    "            spectra = torch.nan_to_num(torch.tensor(file['y'].values, requires_grad=True)).to(torch.float)\n",
    "\n",
    "        #Reshapes it as required\n",
    "        spectra = spectra.reshape(1, 1, 3106)\n",
    "        \n",
    "        #Prevents unknown problem with NaN from before\n",
    "        func_group = torch.nan_to_num(torch.tensor(file['funcGroups'].values[self.func_group_number], requires_grad=True)).to(torch.float)\n",
    "\n",
    "        return spectra, func_group\n",
    "    \n",
    "    def get_nist_id(self, index):\n",
    "        return self.data.iloc[index][0]\n",
    "    \n",
    "    def get_func_groups(self, index):\n",
    "        file_path = os.path.join(preprocDirPath, self.data.iloc[index][0])\n",
    "        file = pd.read_csv(file_path)\n",
    "\n",
    "        return torch.nan_to_num(torch.tensor(file['funcGroups'].values[self.func_group_number], requires_grad=True)).to(torch.float)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        indexes = random.sample(range(self.data.__len__()), n)\n",
    "        return (torch.cat([self.__getitem__(idx)[0] for idx in indexes]).reshape(n, 1, 1, 3106), torch.tensor([self.__getitem__(idx)[1] for idx in indexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_datadir = pd.Series(os.listdir(preprocDirPath), name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_groups_datadirs = {}\n",
    "\n",
    "for name in func_group_names:\n",
    "    functional_groups_datadirs[name] = pd.Series(os.listdir(os.path.join(funcGroupDirPath, name)), name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sample = pd.merge(preprocessed_datadir, functional_groups_datadirs['alkane'], on = [0,0], how = 'outer', indicator = True).query('_merge==\"left_only\"')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_types = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in preprocessed_datadir:\n",
    "    inside = pd.read_csv(os.path.join(preprocDirPath, file))\n",
    "    if inside['spectraType'][0] not in spectra_types.keys():\n",
    "        spectra_types[inside['spectraType'][0]] = 1\n",
    "    else:\n",
    "        spectra_types[inside['spectraType'][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRANSMITTANCE': 2955, 'ABSORBANCE': 5834}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectra_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func_group in func_group_names:\n",
    "    for file in functional_groups_datadirs[func_group]:\n",
    "        inside = pd.read_csv(os.path.join(funcGroupDirPath, func_group, file))\n",
    "        if func_group not in spectra_types.keys():\n",
    "            spectra_types[func_group] = {}\n",
    "\n",
    "        if inside['spectraType'][0] not in spectra_types[func_group].keys():\n",
    "            spectra_types[func_group][inside['spectraType'][0]] = 1\n",
    "        else:\n",
    "            spectra_types[func_group][inside['spectraType'][0]] += 1\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates hashmaps where data is further retrieved by functional group name and its whether its training or test set\n",
    "\n",
    "def createHashmaps(test_ratio: float = 0.3,\n",
    "                    batch_size: int = 128):\n",
    "    \n",
    "    '''\n",
    "    Creates hashmaps containing FTIR data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    test_ratio : float, default 0.3\n",
    "        Ratio of test dataset.\n",
    "    \n",
    "    batch_size : int, default 128\n",
    "        Size of the batch.\n",
    "    '''\n",
    "\n",
    "    func_groups_data, func_groups_datasets, func_groups_dataloaders = {}, {}, {}\n",
    "    for data_directory in os.listdir(funcGroupDirPath):\n",
    "        print(data_directory)\n",
    "        if data_directory==\"carboxylic esters\": \n",
    "            continue\n",
    "        dataset = FunctionalGroupsDataset(data_directory, convert_to = 'absorbance')\n",
    "\n",
    "        training_dataset, test_dataset = random_split(dataset, [1 - test_ratio, test_ratio], torch.Generator())\n",
    "\n",
    "        func_groups_datasets[data_directory] = {'training': training_dataset, 'test': test_dataset}\n",
    "\n",
    "        func_groups_dataloaders[data_directory] = {'training': DataLoader(training_dataset, batch_size = batch_size, shuffle = True), 'test' : DataLoader(test_dataset, batch_size = batch_size, shuffle = False)}\n",
    "\n",
    "    return func_groups_data, func_groups_datasets, func_groups_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_groups_data, func_groups_datasets, func_groups_dataloaders = createHashmaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemicyBad(torch.nn.Module):\n",
    "    def __init__(self, kan_layers: list) -> None:\n",
    "        super(ChemicyBad, self).__init__()\n",
    "\n",
    "        self.tr_accuracy = None\n",
    "        self.te_accuracy = None\n",
    "        self.tr_loss = None\n",
    "        self.te_loss = None\n",
    "\n",
    "        kernel_size_1 = 5\n",
    "        stride_conv_1 = 1\n",
    "\n",
    "        stride_pool_1 = 3\n",
    "        filter_size_1 = 3\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv1d(1, 10, kernel_size=kernel_size_1, stride=stride_conv_1)\n",
    "\n",
    "        self.pool_1 = torch.nn.MaxPool1d(filter_size_1, stride_pool_1)\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv1d(10, 10, kernel_size=kernel_size_1, stride=stride_conv_1)\n",
    "\n",
    "        self.pool_2 = torch.nn.MaxPool1d(filter_size_1, stride_pool_1)\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv1d(10, 10, kernel_size=kernel_size_1, stride=stride_conv_1)\n",
    "\n",
    "        self.pool_3 = torch.nn.MaxPool1d(filter_size_1, stride_pool_1)\n",
    "\n",
    "        self.kan = KAN([1130] + kan_layers).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, h, w = x.shape\n",
    "        x = x.reshape(bs, c, h*w)\n",
    "        out = self.conv_1(x)\n",
    "        out = F.tanh(out)\n",
    "        out = self.pool_1(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = F.tanh(out)\n",
    "        out = self.pool_2(out)\n",
    "        \n",
    "        out = self.conv_3(out)\n",
    "        out = F.tanh(out)\n",
    "        out = self.pool_3(out)\n",
    "\n",
    "        out = out.reshape(x.shape[0],1,out.shape[2]*out.shape[1])\n",
    "\n",
    "        out = self.kan(out)\n",
    "        out = F.sigmoid(out)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "    \n",
    "def train(num_epochs, lossFn, optimizer, group: str, weight_decay, learning_rate, kan_layers, lambda_lr, seed=42, save: bool = False, plot: bool = False, disable_verbose: bool = False, iter=0, dirpath=\"C:\\\\Users\\\\tomek\\\\Desktop\\\\cc\\\\res\"):\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "\n",
    "    model = ChemicyBad(kan_layers).to(device)\n",
    "\n",
    "    #torch.manual_seed(seed)\n",
    "\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: lambda_lr ** epoch)\n",
    "    #scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        with tqdm(func_groups_dataloaders[group]['training'], disable=disable_verbose) as pbar:\n",
    "            f1 = 0\n",
    "            lista_grup = np.zeros(1)\n",
    "            list_func = np.zeros(1)\n",
    "            for y_batch, func_group_batch in pbar:\n",
    "                y_batch, func_group_batch = y_batch.to(device), func_group_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(y_batch).squeeze()\n",
    "                loss = lossFn(output, func_group_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                for i in range(output.shape[0]):\n",
    "                    #F1 metryka\n",
    "                    predicted_well = ((torch.round(output[i]) == func_group_batch[i]) * 1 * func_group_batch[i]).item()\n",
    "\n",
    "                    lista_grup += predicted_well\n",
    "                    list_func += func_group_batch[i].item()\n",
    "\n",
    "                f1 = metrics.f1_score(torch.Tensor.cpu(func_group_batch).detach().numpy(), torch.Tensor.cpu(torch.round(output)).detach().numpy())\n",
    "                pbar.set_postfix(epoch=epoch, loss=train_loss, accuracy=(lista_grup/list_func).item(), f1=f1, lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "                train_acc = (lista_grup/list_func).item()\n",
    "            train_loss /= len(func_groups_dataloaders[group]['training'])\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(func_groups_dataloaders[group]['test'], disable=disable_verbose) as tbar:\n",
    "                f1 = 0\n",
    "                lista_grup = np.zeros(1)\n",
    "                list_func = np.zeros(1)\n",
    "                for y_batch, func_group_batch in tbar:\n",
    "                    y_batch, func_group_batch = y_batch.to(device), func_group_batch.to(device)\n",
    "\n",
    "                    output = model(y_batch).squeeze()\n",
    "                    loss = lossFn(output, func_group_batch)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    for i in range(output.shape[0]):\n",
    "                        predicted_well = ((torch.round(output[i]) == func_group_batch[i]) * 1 * func_group_batch[i]).item()\n",
    "\n",
    "                        lista_grup += predicted_well\n",
    "                        list_func += func_group_batch[i].item()\n",
    "                    f1 = metrics.f1_score(torch.Tensor.cpu(func_group_batch).detach().numpy(), torch.Tensor.cpu(torch.round(output)).detach().numpy())\n",
    "                    tbar.set_postfix(epoch=epoch, loss=test_loss, accuracy=(lista_grup/list_func).item(), f1=f1)\n",
    "\n",
    "                    test_acc = (lista_grup/list_func).item()\n",
    "                test_loss /= len(func_groups_dataloaders[group]['test'])\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "    #if save:\n",
    "    #    torch.save(model, os.path.join('..','ALL', 'trained_models', group, f'{group}_{time.strftime(r\"%m:%d:%Y, %H:%M:%S\")}.pt'))\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(train_losses, label='training loss')\n",
    "        plt.plot(test_losses, label='test loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'{group}')\n",
    "        plt.plot()\n",
    "        plt.savefig(dirpath+\"\\\\Learning_progress_\"+str(iter)+\".png\")\n",
    "        plt.show()\n",
    "\n",
    "    model.tr_accuracy = np.round(train_acc,2)\n",
    "    model.te_accuracy = np.round(test_acc,2)\n",
    "    model.tr_loss = np.round(train_losses[-1],2)\n",
    "    model.te_loss = np.round(test_losses[-1],2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadding CNN-KAN pretrained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = {}\n",
    "print(trained_models_path)\n",
    "for model_path in os.listdir(trained_models_path):\n",
    "    model_name = model_path.split('_')[0]\n",
    "    loaded_models[model_name] = torch.load(os.path.join(trained_models_path, model_path), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_models: dict[str, shap.GradientExplainer] = {}\n",
    "\n",
    "for func_group in func_groups_dataloaders.keys():\n",
    "    #if func_group!='alkane' and func_group!='aromatics' and func_group!='nitriles' and func_group!='alcohols' and func_group!='ketones' and func_group!='nitro' and func_group!='phenols' and func_group!='carboxylic acids':\n",
    "    #    continue\n",
    "    for idx, (spectra, func_groups) in enumerate(func_groups_dataloaders[func_group]['training']):\n",
    "        if idx != 0:\n",
    "            full_test_spectra = torch.cat((full_test_spectra, spectra))  # noqa: F821\n",
    "        else:\n",
    "            full_test_spectra = spectra\n",
    "    zzz=full_test_spectra[0:1000,:,:,:]\n",
    "    explainer_models[func_group] = shap.GradientExplainer(\n",
    "        loaded_models[func_group].cpu(),\n",
    "        zzz,\n",
    "        batch_size=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_values(func_group: str, target_func_group: str) -> tuple[np.array, torch.tensor, torch.tensor]:\n",
    "    '''\n",
    "    Returns SHAP values, functional groups and full spectras of a specific functional group. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    func_group : str\n",
    "        Name of the functional group.\n",
    "    \n",
    "    target_func_group : str\n",
    "        Name of the target functional group.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    for idx, (spectra, func_groups) in enumerate(func_groups_dataloaders[target_func_group]['test']):\n",
    "        if idx != 0:\n",
    "            full_test_spectra = torch.cat((full_test_spectra, spectra))\n",
    "            full_func_groups = torch.cat((full_func_groups, func_groups))\n",
    "        else:\n",
    "            full_test_spectra = spectra\n",
    "            full_func_groups = func_groups\n",
    "        \n",
    "    \n",
    "    return explainer_models[func_group].shap_values(full_test_spectra[0:101,:,:,:], nsamples=100), full_func_groups, full_test_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_shaps: dict[str, np.array] = {}\n",
    "shap_func_groups: dict[str, torch.tensor] = {}\n",
    "full_test_spectras: dict[str, torch.tensor] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all required files exist\n",
    "if not all(file in os.listdir(save_dir) for file in [\"explainer_shaps.pkl\", \"shap_func_groups.pkl\", \"full_test_spectras.pkl\"]):\n",
    "    # Calculate SHAP values for each functional group\n",
    "    for func_group in func_groups_dataloaders.keys():\n",
    "        print(func_group)\n",
    "        #if func_group!='alkane' and func_group!='aromatics' and func_group!='nitriles' and func_group!='alcohols' and func_group!='ketones' and func_group!='nitro' and func_group!='phenols' and func_group!='carboxylic acids':\n",
    "        #if func_group!='nitriles' :\n",
    "        #    print(\"pomijam\")\n",
    "        #    continue\n",
    "        explainer_shaps[func_group], shap_func_groups[func_group], full_test_spectras[func_group] = get_shap_values(func_group, func_group)\n",
    "    \n",
    "    # Save the data as pickle files\n",
    "    import pickle\n",
    "    with open(os.path.join(save_dir, \"explainer_shaps.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(explainer_shaps, f)\n",
    "    with open(os.path.join(save_dir, \"shap_func_groups.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_func_groups, f)\n",
    "    with open(os.path.join(save_dir, \"full_test_spectras.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(full_test_spectras, f)\n",
    "else:\n",
    "    # Load the data from pickle files\n",
    "    import pickle\n",
    "    with open(os.path.join(save_dir, \"explainer_shaps.pkl\"), \"rb\") as f:\n",
    "        explainer_shaps = pickle.load(f)\n",
    "    with open(os.path.join(save_dir, \"shap_func_groups.pkl\"), \"rb\") as f:\n",
    "        shap_func_groups = pickle.load(f)\n",
    "    with open(os.path.join(save_dir, \"full_test_spectras.pkl\"), \"rb\") as f:\n",
    "        full_test_spectras = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_values(idx, functional_group, shap_func_groups, shap_values: dict[str, np.array], save: bool, shap_limit: float) -> None:\n",
    "    x = range(670, 3776)\n",
    "    \n",
    "    y = func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_prediction = loaded_models[functional_group].forward(func_groups_datasets[functional_group]['test'][idx][0].reshape(1,1,1,3106))[0][0]\n",
    "\n",
    "    plt.plot(\n",
    "        x,\n",
    "        y,\n",
    "        alpha=0.2,\n",
    "        c='grey'\n",
    "    )\n",
    "   \n",
    "    plt.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        c=shap_values[functional_group][idx].flatten(),\n",
    "        cmap='bwr',\n",
    "        marker='D',\n",
    "        s=4,\n",
    "        norm=colors.TwoSlopeNorm(vcenter=0, vmin=-shap_limit, vmax=shap_limit)\n",
    "    )\n",
    "    #plt.title(f\"SHAP values for {\"\" if model_prediction >= 0.5 else \"non\"} {functional_group} object\\nActual group: {shap_func_groups[functional_group][idx].item()}\\nModel prediction: {model_prediction:.2f}\\n{func_groups_datasets[functional_group]['test'].dataset.get_nist_id(func_groups_datasets[functional_group]['test'].indices[idx]).split('.')[0]}\")\n",
    "    plt.title(f'SHAP values for {\"\" if model_prediction >= 0.5 else \"non\"}   {functional_group}  object\\nActual group: {shap_func_groups[functional_group][idx].item()}  \\n\\nModel prediction: {model_prediction:.2f}  ')\n",
    "   \n",
    "    plt.ylabel(\"Normalized absorbance\")\n",
    "    plt.xlabel(r\"Wavenumber [cm$^{-1}$]\")\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    if save:\n",
    "        plt.savefig('C:\\\\Users\\\\Tomek_\\\\Desktop\\\\rys1\\\\fig' +str(idx)+'.png', format='png',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot shap values for spectrum with idx=52 for nitriles functional group\n",
    "plot_shap_values(52, \n",
    "                     'nitriles', \n",
    "                     shap_func_groups, \n",
    "                     explainer_shaps, \n",
    "                     False, \n",
    "                     0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which add SHAP values for specified spectrum for characteristic regions for different functional groups\n",
    "\n",
    "def sumator(idx, functional_group, shap_func_groups, shap_values: dict[str, np.array], save: bool, shap_limit: float) -> None:\n",
    "       \n",
    "    x = range(670, 3776)        \n",
    "    if functional_group=='alkane':\n",
    "        x=list(range(1340,1395))\n",
    "        x1=list(range(1430,1480))  \n",
    "        x2=list(range(2850,2990)) #alkane\n",
    "        x=x+x1+x2        \n",
    "    if functional_group=='alkene':\n",
    "        x=list(range(685,995))\n",
    "        x1=list(range(1600,1680))  \n",
    "        x2=list(range(3000,3100)) #alkene\n",
    "        x=x+x1+x2        \n",
    "    if functional_group=='alkyne':\n",
    "        x=list(range(2100,2250))\n",
    "        x1=list(range(3200,3310)) #alkyne        \n",
    "        x=x+x1        \n",
    "    if functional_group=='aromatics':\n",
    "        x=list(range(680,900))\n",
    "        x1=list(range(1440,1620))  \n",
    "        x2=list(range(3000,3100)) #romatics\n",
    "        x=x+x1+x2        \n",
    "    if functional_group=='alcohols':\n",
    "        x=list(range(1000,1300))\n",
    "        x1=list(range(3200,3650)) #       \n",
    "        x=x+x1         \n",
    "    if functional_group==\"amines\":\n",
    "        x=list(range(3250,3550))        \n",
    "    if functional_group=='nitriles':\n",
    "        x = range(2200, 2280) #nitryle\n",
    "\n",
    "    if functional_group=='aldehydes':    \n",
    "        x=list(range(1680,1715)) #       \n",
    "        x1=list(range(1720,1740)) #  \n",
    "        x2=list(range(2700,2900))\n",
    "        x=x+x1+x2\n",
    "    if functional_group=='ketones':\n",
    "        x=list(range(1650,1700)) #ketones\n",
    "        x1=list(range(1705,1750)) #ketones\n",
    "        x=x+x1\n",
    "    if functional_group=='esters':\n",
    "        x=list(range(1000,1300))\n",
    "        x1=list(range(1715,1730)) #    \n",
    "        x2=list(range(1735,1765)) #\n",
    "        x=x+x1+x2\n",
    "    if functional_group=='carboxylic acids':\n",
    "        x=list(range(1000,1300))\n",
    "        x1=list(range(1680,1725))\n",
    "        x2=list(range(2500,3200))\n",
    "        x=x+x1+x2\n",
    "    if functional_group=='amides':\n",
    "        x=list(range(1630,1700))\n",
    "        x1=list(range(3150,3500)) #       \n",
    "        x=x+x1\n",
    "    if functional_group=='nitro':\n",
    "        x=list(range(1300,1390))\n",
    "        x1=list(range(1490,1570)) #ketones       \n",
    "        x=x+x1        \n",
    "    if functional_group=='phenols':\n",
    "        x=range(3200,3700) #phenols\n",
    "    if functional_group=='methyl':\n",
    "        x=list(range(1365,1395))\n",
    "        x1=list(range(1430,1470)) #    \n",
    "        x2=list(range(2860,2880)) #\n",
    "        x3=list(range(2950,2970)) #\n",
    "        x=x+x1+x2+x3\n",
    "\n",
    "    if functional_group=='alkane':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[670:725]) #alkane\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[760:810]) #alkane        \n",
    "        y2 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2180:2320]) #alkane\n",
    "        y=y+y1+y2\n",
    "        \n",
    "    if functional_group=='alkene':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[15:325]) #alkene\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[930:1010]) #alkene        \n",
    "        y2 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2330:2430]) #alkene\n",
    "        y=y+y1+y2\n",
    "    if functional_group=='alkyne':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1430:1580]) #alkyne\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2530:2640]) #alkyne      \n",
    "        y=y+y1\n",
    "    if functional_group=='aromatics':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[10:230]) #aromatics\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[770:950]) #aromatics       \n",
    "        y2 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2330:2430]) #aromatics\n",
    "        y=y+y1+y2\n",
    "    if functional_group=='alcohols':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[330:630])\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2530:2980]) #\n",
    "        y=y+y1  \n",
    "    if functional_group=='amines':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2580:2880])\n",
    "    if functional_group=='nitriles':\n",
    "        y = func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1530:1610]  #nitryles\n",
    "    if functional_group=='aldehydes':    \n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1010:1045]) #aldehydes\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1050:1070]) #aldehydes       \n",
    "        y2 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2030:2230]) #aldehydes\n",
    "        y=y+y1+y2\n",
    "    if functional_group=='ketones':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[980:1030]) #ketones\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1035:1080]) #ketones\n",
    "        y=y+y1\n",
    "    if functional_group=='esters':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[330:630])\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1045:1060]) #\n",
    "        y2 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1065:1095]) #nitro\n",
    "        y=y+y1+y2\n",
    "    if functional_group=='carboxylic acids':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[330:630])\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1010:1055]) #\n",
    "        y2 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[1830:2530]) #nitro\n",
    "        y=y+y1+y2\n",
    "    if functional_group=='amides':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[960:1030])\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2480:2830]) #\n",
    "        y=y+y1  \n",
    "    if functional_group=='nitro':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[630:720])\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[820:900]) #nitro\n",
    "        y=y+y1 \n",
    "    if functional_group=='phenols':\n",
    "        y = func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2530:3030] #phenols\n",
    "    if functional_group=='methyl':\n",
    "        y = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[695:725])\n",
    "        y1 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[760:800]) #\n",
    "        y2 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2190:2210]) #n\n",
    "        y3 = list(func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()[2280:2300]) #n\n",
    "        y=y+y1+y2+y3\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_prediction = loaded_models[functional_group].forward(func_groups_datasets[functional_group]['test'][idx][0].reshape(1,1,1,3106))[0][0]\n",
    "    #print(x)\n",
    "    plt.plot(\n",
    "        x,\n",
    "        y,\n",
    "        alpha=0.2,\n",
    "        c='grey'\n",
    "    )\n",
    "    \n",
    "    ccc='x',\n",
    "    if functional_group=='alkane':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[670:725]) #alkanes\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[760:810]) #alkanes\n",
    "        ccc2=list(shap_values[functional_group][idx].flatten()[2180:2320]) #alkanes\n",
    "        ccc=ccc+ccc1+ccc2\n",
    "    if functional_group=='alkene':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[15:325]) #alkanes\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[930:1010]) #alkanes\n",
    "        ccc2=list(shap_values[functional_group][idx].flatten()[2330:2430]) #alkanes\n",
    "        ccc=ccc+ccc1+ccc2\n",
    "    if functional_group=='alkyne':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[1430:1580]) #alkenes\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[2530:2640]) #alkenes        \n",
    "        ccc=ccc+ccc1\n",
    "    if functional_group=='aromatics':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[10:230]) #aromatics\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[770:950]) #aromatics\n",
    "        ccc2=list(shap_values[functional_group][idx].flatten()[2330:2430]) #aromatics\n",
    "        ccc=ccc+ccc1+ccc2\n",
    "    if functional_group=='alcohols':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[330:630]) #n\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[2530:2980]) #\n",
    "        ccc=ccc+ccc1\n",
    "    if functional_group=='amines':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[2580:2880]) #amines        \n",
    "    if functional_group=='nitriles':    \n",
    "        ccc=shap_values[functional_group][idx].flatten()[1530:1610] #nitriles\n",
    "    if functional_group=='aldehydes':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[1010:1045]) #aldehydes\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[1050:1070]) #aldehydes\n",
    "        ccc2=list(shap_values[functional_group][idx].flatten()[2030:2230]) #aldehydes\n",
    "        ccc=ccc+ccc1+ccc2\n",
    "    if functional_group=='ketones':    \n",
    "        ccc=shap_values[functional_group][idx].flatten()[980:1030] #ketones\n",
    "        ccc1=shap_values[functional_group][idx].flatten()[1035:1080] #ketones\n",
    "        ccc=ccc+ccc1\n",
    "    if functional_group=='esters':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[330:630]) #n\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[1045:1060]) #\n",
    "        ccc2=list(shap_values[functional_group][idx].flatten()[1065:1095]) #\n",
    "        ccc=ccc+ccc1+ccc2\n",
    "    if functional_group=='carboxylic acids':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[330:630]) #n\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[1010:1055]) #\n",
    "        ccc2=list(shap_values[functional_group][idx].flatten()[1830:2530]) #\n",
    "        ccc=ccc+ccc1+ccc2\n",
    "    if functional_group=='amides':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[960:1030]) #n\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[2480:2830]) #\n",
    "        ccc=ccc+ccc1\n",
    "    if functional_group=='nitro':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[630:720]) #nitro\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[820:900]) #nitro\n",
    "        ccc=ccc+ccc1 \n",
    "    if functional_group=='phenols':    \n",
    "        ccc=shap_values[functional_group][idx].flatten()[2530:3030] #phenols\n",
    "    if functional_group=='methyl':\n",
    "        ccc=list(shap_values[functional_group][idx].flatten()[695:725]) #n\n",
    "        ccc1=list(shap_values[functional_group][idx].flatten()[760:800]) #\n",
    "        ccc2=list(shap_values[functional_group][idx].flatten()[2190:2210]) #\n",
    "        ccc3=list(shap_values[functional_group][idx].flatten()[2280:2300]) #\n",
    "        ccc=ccc+ccc1+ccc2+ccc3\n",
    "        \n",
    "    #print(np.sum(ccc))\n",
    "    plt.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        c=ccc,\n",
    "        cmap='bwr',\n",
    "        marker='D',\n",
    "        s=4,\n",
    "        norm=colors.TwoSlopeNorm(vcenter=0, vmin=-shap_limit, vmax=shap_limit)\n",
    "    )\n",
    "    return(np.sum(ccc));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMean(fg,posneg,onlyCorrectDec):\n",
    "    sumka=0.0;\n",
    "    iter=0;\n",
    "    for idx in range(100): \n",
    "        \n",
    "        z=shap_func_groups[fg][idx].item()\n",
    "        model_prediction = loaded_models[fg].forward(func_groups_datasets[fg]['test'][idx][0].reshape(1,1,1,3106))[0][0]\n",
    "        r=(np.round(model_prediction.item()))\n",
    "        co=abs(r-z)<0.01;\n",
    "        war=False;\n",
    "        if posneg=='positive' and onlyCorrectDec==False:\n",
    "            war= (z==1.0)\n",
    "        if posneg=='negative' and onlyCorrectDec==False:\n",
    "            war= (z==0.0)\n",
    "        if posneg=='positive' and onlyCorrectDec==True:\n",
    "            war= (z==1.0 and model_prediction>=0.5)\n",
    "        if posneg=='negative' and onlyCorrectDec==True:\n",
    "            war= (z==0.0 and model_prediction<0.5)\n",
    "                   \n",
    "        if war:\n",
    " \n",
    "            iter=iter+1\n",
    "            wklad=sumator(idx,   fg,   shap_func_groups,    explainer_shaps, True,     0.002)\n",
    "            sumka=sumka+wklad;\n",
    "            #print(idx,np.round(wklad,3),co)\n",
    "    \n",
    "    print('sumka',sumka)\n",
    "    srednia=sumka/iter\n",
    "    print('srednia',srednia);\n",
    "    print('iter',iter)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.show()\n",
    "import random\n",
    "#fg='alcohols'\n",
    "#fg='aldehydes'\n",
    "#fg='alkane'\n",
    "fg='alkene'\n",
    "#fg='alkyne'\n",
    "#fg='amides'\n",
    "#fg='amines'\n",
    "#fg='aromatics'\n",
    "#fg='carboxylic acids'\n",
    "#fg='methyl'\n",
    "#fg='phenols'\n",
    "#fg='ketones'\n",
    "#fg='nitriles'\n",
    "#fg='esters'\n",
    "#fg='nitro'\n",
    "#fg='aromatics'\n",
    "\n",
    "print('All positive')\n",
    "calcMean(fg,'positive',False)\n",
    "\n",
    "print('All negative')\n",
    "calcMean(fg,'negative',False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate the accuracy (in %) for given dataset\n",
    "print('accuracy of CNN-KAN model (in%)  for given subset of spectra for ',fg)\n",
    "import math\n",
    "correct=0;\n",
    "for idx in range(100):         \n",
    "        z=shap_func_groups[fg][idx].item()\n",
    "        model_prediction = loaded_models[fg].forward(func_groups_datasets[fg]['test'][idx][0].reshape(1,1,1,3106))[0][0].item()\n",
    "        r=(np.round(model_prediction))\n",
    "        if abs(r-z)<0.01:\n",
    "            correct+=1;\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcToPlot1(functional_group):\n",
    "    sumka=[]\n",
    "    sumkaPlus=[];\n",
    "    sumkaMinus=[];\n",
    "    firstPlus=True\n",
    "    firstMinus=True\n",
    "    for idx in range(100):     \n",
    "        z=shap_func_groups[functional_group][idx].item()  \n",
    "        if z==1.0: \n",
    "            if firstPlus==True:\n",
    "                #sumka=explainer_shaps[functional_group][idx].flatten()\n",
    "                sumkaPlus=explainer_shaps[functional_group][idx].flatten()\n",
    "                #print(sumka.shape)\n",
    "                firstPlus=False;\n",
    "            else:            \n",
    "                #sumka=-1.0*explainer_shaps[functional_group][idx].flatten()\n",
    "                sumkaPlus+=explainer_shaps[functional_group][idx].flatten()\n",
    "        else:\n",
    "            if firstMinus==True:           \n",
    "                #sumka+=explainer_shaps[functional_group][idx].flatten()\n",
    "                sumkaMinus=-1*explainer_shaps[functional_group][idx].flatten()\n",
    "                firstMinus=False;\n",
    "            else: \n",
    "                #sumka-=explainer_shaps[functional_group][idx].flatten()\n",
    "                sumkaMinus-=explainer_shaps[functional_group][idx].flatten()     \n",
    "    x = range(670, 3776)\n",
    "    return [x,sumkaPlus,sumkaMinus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot S_+,S_- and S for four functional groups (article Fig. 5) \n",
    "\n",
    "plt.clf()\n",
    "plt.show()\n",
    "import random\n",
    "func_gr=['nitriles','nitro','phenols','ketones','carboxylic acids']\n",
    "\n",
    "labelFontSize=16\n",
    "labelBigFontSize=18\n",
    "labelAxisSize=14\n",
    "res=[];\n",
    "\n",
    "for i in range(len(func_gr)):\n",
    "    res.append(calcToPlot1(func_gr[i]));\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(func_gr), ncols=1, figsize=(9, 15),constrained_layout=True)\n",
    "\n",
    "\n",
    "# clear subplots\n",
    "for ax in axs:\n",
    "    ax.remove()\n",
    "\n",
    "# add subfigure per subplot\n",
    "gridspec = axs[0].get_subplotspec().get_gridspec()\n",
    "subfigs = [fig.add_subfigure(gs) for gs in gridspec]\n",
    "\n",
    "for row, subfig in enumerate(subfigs):\n",
    "    subfig.suptitle(func_gr[row].capitalize(),fontsize=labelBigFontSize)\n",
    "\n",
    "    # create 1x3 subplots per subfig\n",
    "    axs = subfig.subplots(nrows=1, ncols=3,sharey='row')\n",
    "    temp=res[row]\n",
    "    for col, ax in enumerate(axs):\n",
    "        if col==0:\n",
    "            ax.plot(temp[0],temp[1],color='salmon')\n",
    "            ax.tick_params(labelsize=labelAxisSize)\n",
    "        if col==1:\n",
    "            ax.plot(temp[0],temp[2],color='cornflowerblue')\n",
    "            ax.tick_params(labelsize=labelAxisSize)\n",
    "        if col==2:\n",
    "            ax.plot(temp[0],temp[1]+temp[2],color='mediumseagreen')\n",
    "            ax.tick_params(labelsize=labelAxisSize)\n",
    "  \n",
    "\n",
    "fig.text(0.5, -0.025, 'Wavenumber [cm$^{-1}$]', ha='center',fontsize=labelFontSize)\n",
    "fig.text(-0.025, 0.5, 'Regions of spectra important for detection of given functional group [arb.units]', va='center', rotation='vertical',fontsize=labelFontSize)\n",
    "\n",
    "#Export plot as png file to specified directory\n",
    "fig.savefig('C:\\\\Users\\\\Tomek_\\\\Desktop\\\\explainerArticle\\\\pp.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different version of function for plotting SHAP values for given spectrum (without visible colormap), other aspect ratio and fonts (usefull for creaction  of figures for article)\n",
    "def plot_shap_valuesNV(idx, functional_group, shap_func_groups, shap_values: dict[str, np.array], save: bool, shap_limit: float) -> None:\n",
    "    from matplotlib import ticker\n",
    "    x = range(670, 3776)\n",
    "    #print(functional_group)\n",
    "    #print(func_groups_datasets.keys())\n",
    "    #print(shap_values.keys())\n",
    "    y = func_groups_datasets[functional_group]['test'][idx][0].flatten().detach().numpy()\n",
    "\n",
    "    labelFontSize=16\n",
    "    labelBigFontSize=18\n",
    "    labelAxisSize=14\n",
    "    labelColorBarFontSize=14\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_prediction = loaded_models[functional_group].forward(func_groups_datasets[functional_group]['test'][idx][0].reshape(1,1,1,3106))[0][0]\n",
    "\n",
    "    plt.plot(\n",
    "        x,\n",
    "        y,\n",
    "        alpha=0.55,\n",
    "        c='dimgrey'\n",
    "    )\n",
    "    #print('aa',shap_values[functional_group][idx].shape)\n",
    "    #print('xx',shap_values[functional_group][idx].flatten().shape)\n",
    "\n",
    "    plt.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        c=shap_values[functional_group][idx].flatten(),\n",
    "        cmap='bwr',\n",
    "        marker='D',\n",
    "        s=4,\n",
    "        norm=colors.TwoSlopeNorm(vcenter=0, vmin=-shap_limit, vmax=shap_limit)\n",
    "    )\n",
    "    #plt.title(f\"SHAP values for {\"\" if model_prediction >= 0.5 else \"non\"} {functional_group} object\\nActual group: {shap_func_groups[functional_group][idx].item()}\\nModel prediction: {model_prediction:.2f}\\n{func_groups_datasets[functional_group]['test'].dataset.get_nist_id(func_groups_datasets[functional_group]['test'].indices[idx]).split('.')[0]}\")\n",
    "    plt.title(f'SHAP values for {\"\" if model_prediction >= 0.5 else \"non\"}   {functional_group}  object\\nActual group: {shap_func_groups[functional_group][idx].item()}  \\n\\nModel prediction: {model_prediction:.2f}  ', fontsize=labelFontSize)\n",
    "    plt.tick_params(labelsize=labelAxisSize)\n",
    "    #plt.ylabel(\"Normalized absorbance\",fontsize=labelBigFontSize)\n",
    "    #plt.xlabel(r\"Wavenumber [cm$^{-1}$]\",fontsize=labelBigFontSize)\n",
    "    plt.tight_layout()\n",
    "    if False:\n",
    "        colbar=plt.colorbar(orientation='horizontal')\n",
    "        colbar.ax.tick_params(labelsize=labelColorBarFontSize)   \n",
    "        colbar.set_ticks([-0.002,-0.001,0,0.001,0.002])\n",
    "    plt.xticks(fontsize=labelAxisSize)\n",
    "    plt.yticks(fontsize=labelAxisSize)\n",
    "    if save:\n",
    "        plt.savefig('C:\\\\Users\\\\Tomek_\\\\Desktop\\\\rysKetonesNW1\\\\fig' +str(idx)+'.png', format='png',dpi=300,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "fg='ketones'\n",
    "for a in range(1): \n",
    "    i=random.randint(0,100)\n",
    "    plot_shap_valuesNV(i, \n",
    "                     fg,\n",
    "                     shap_func_groups, \n",
    "                     explainer_shaps, \n",
    "                     False, \n",
    "                     0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
